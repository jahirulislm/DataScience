{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five importnat ways for Imputing Missing Values\n",
    "\n",
    "You can impute missing values using machine learning models. This process is known as data imputation and is commonly used in data preprocessing to handle missing or incomplete data. There are several methods and models you can use, depending on the nature of your data and the missing values:\n",
    "\n",
    "1. **`Simple Imputation Techniques:`** \n",
    "   - **Mean/Median Imputation:** Replace missing values with the mean or median of the column. Suitable for numerical data.\n",
    "   - **Mode Imputation:** Replace missing values with the mode (most frequent value) of the column. Useful for categorical data.\n",
    "\n",
    "2. **`K-Nearest Neighbors (KNN)`:** This algorithm can be used to impute missing values based on the similarity of rows.\n",
    "\n",
    "3. **`Regression Imputation:`** Use a regression model to predict the missing values based on other variables in your dataset.\n",
    "\n",
    "4. **`Decision Trees and Random Forests:`** These can handle missing values inherently. They can also be used to predict missing values based on the patterns learned from the other data.\n",
    "\n",
    "5. **`Advanced Techniques:`**\n",
    "   - **Multiple Imputation by Chained Equations (MICE):** This is a more sophisticated technique that models each variable with missing values as a function of other variables in a round-robin fashion.\n",
    "   - **Deep Learning Methods:** Neural networks, especially autoencoders, can be effective in imputing missing values in complex datasets.\n",
    "\n",
    "6. **`Time Series Specific Methods:`** For time-series data, you might use techniques like interpolation, forward-fill, or backward-fill.\n",
    "\n",
    "It's important to choose the right method based on the type of data, the pattern of missingness (e.g., at random, completely at random, or not at random), and the amount of missing data. Additionally, it's crucial to understand that imputation can introduce bias or affect the distribution of your data, so it should be done with caution and an understanding of the potential implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Imputation Techniques\n",
    "\n",
    "## 1.1. Mean/Median Imputation\n",
    "\n",
    "Mean/median imputation replaces missing values with the mean or median of the column. This is a simple and effective method, but it has some limitations. For example, it reduces variance in the dataset, and it can lead to biased estimates if the missing values are not missing at random.\n",
    "\n",
    "Let's see how to implement mean/median imputation in Python using the Titanic dataset.\n",
    "\n",
    "### 1.1.1. Mean Imputation\n",
    "\n",
    "First, let's import the necessary libraries and load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# load the Titanic dataset\n",
    "data = sns.load_dataset('titanic')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of missing values in each column\n",
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `age` column has 177 missing values. Let's replace these missing values with the mean of the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with mean\n",
    "data['age'] = data['age'].fillna(data['age'].mean())\n",
    "\n",
    "# check the number of missing values in each column\n",
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the missing values in the `age` column have been replaced with the mean of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Median Imputation\n",
    "\n",
    "Let's load the dataset and replace the missing values in the `age` column with the median of the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with median\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "\n",
    "# check the number of missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Mode Imputation\n",
    "\n",
    "Mode imputation replaces missing values with the mode (most frequent value) of the column. This is useful for imputing categorical columns, such as `Embarked` and `embark_town` in the Titanic dataset.\n",
    "\n",
    "Let's see how to implement mode imputation in Python using the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# check the number of missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with mode\n",
    "df['embark_town'] = df['embark_town'].fillna(df['embark_town'].mode()[0])\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "\n",
    "# check the number of missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the missing values in the `embark_town` column and `embarked` column have been replaced with the mode of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN is a machine learning algorithm that can be used for imputing missing values. It works by finding the most similar data points to the one with the missing value based on other available features. The missing value is then imputed with the mean or median of the most similar data points.\n",
    "\n",
    "Let's see how to implement KNN imputation in Python using the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# check the number of missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with KNN imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# call the KNN class with number of neighbors = 4\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "\n",
    "#impute missing values with KNN imputer\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "# check the number of missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression Imputation\n",
    "\n",
    "Regression imputation uses a regression model to predict the missing values based on other variables in the dataset. It works well for both categorical and numerical data.\n",
    "\n",
    "Let's see how to implement regression imputation in Python using the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# check the number of missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with regression imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# call the IterativeImputer class with max_iter = 10\n",
    "imputer = IterativeImputer(max_iter=10)\n",
    "\n",
    "#impute missing values with regression imputer\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "# check the number of missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random Forests for Imputing Missing Values\n",
    "\n",
    "Random forests can handle missing values inherently. They can also be used to predict missing values based on the patterns learned from the other data.\n",
    "\n",
    "Let's see how to implement random forests in Python using the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1. load the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# check missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the deck column from the dataset because it has too many missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove deck column\n",
    "df.drop('deck', axis=1, inplace=True)\n",
    "\n",
    "# check missing values in each column\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encode the data at this stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the data using label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Columns to encode\n",
    "columns_to_encode = ['sex', 'embarked', 'who', 'class', 'embark_town', 'alive']\n",
    "\n",
    "# Dictionary to store LabelEncoders for each column\n",
    "label_encoders = {}\n",
    "\n",
    "# Loop to apply LabelEncoder to each column\n",
    "for col in columns_to_encode:\n",
    "    # Create a new LabelEncoder for the column\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Fit and transform the data, then inverse transform it\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    # Store the encoder in the dictionary\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Check the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first impute the missing values in the age column before we can use it to predict the missing values in the `embarked` and `emark_town` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into two parts: one with missing values, one without\n",
    "df_with_missing = df[df['age'].isna()]\n",
    "# dropna removes all rows with missing values\n",
    "df_without_missing = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the shape of the datasets with and without the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the original dataset is: \", df.shape)\n",
    "print(\"The shape of the dataset with missing values removed is: \", df_without_missing.shape)\n",
    "print(\"The shape of the dataset with missing values is: \", df_with_missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see the first five rows of the dataset with the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see the first five rows of the dataset without the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the names of all the columns in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the names of the columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Imputation\n",
    "\n",
    "# split the data into X and y and we will only take the columns with no missing values\n",
    "X = df_without_missing.drop(['age'], axis=1)\n",
    "y = df_without_missing['age']\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Random Forest Imputation\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"RMSE for Random Forest Imputation: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R2 Score for Random Forest Imputation: \", r2_score(y_test, y_pred))\n",
    "print(\"MAE for Random Forest Imputation: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MAPE for Random Forest Imputation: \", mean_absolute_percentage_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of missing values in each column\n",
    "df_with_missing.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict missing values\n",
    "y_pred = rf_model.predict(df_with_missing.drop(['age'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# replace the missing values with the predicted values\n",
    "df_with_missing['age'] = y_pred\n",
    "\n",
    "# check the missing values\n",
    "df_with_missing.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two dataframes\n",
    "df_complete = pd.concat([df_with_missing, df_without_missing], axis=0)\n",
    "# print the shape of the complete dataframe\n",
    "print(\"The shape of the complete dataframe is: \", df_complete.shape)\n",
    "\n",
    "#check the first 5 rows of the complete dataframe\n",
    "df_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_to_encode:\n",
    "    # Retrieve the corresponding LabelEncoder for the column\n",
    "    le = label_encoders[col]\n",
    "\n",
    "    # Inverse transform the data\n",
    "    df_complete[col] = le.inverse_transform(df[col])\n",
    "    \n",
    "# check the first 5 rows of the complete dataframe\n",
    "df_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the complete dataframe\n",
    "print(\"The shape of the complete dataframe is: \", df_complete.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of missing values in each column\n",
    "df_complete.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Advanced Techniques\n",
    "\n",
    "## 5.1. Multiple Imputation by Chained Equations (MICE)\n",
    "\n",
    "Multiple Imputation by Chained Equations (MICE) is a more sophisticated technique that models each variable with missing values as a function of other variables in a round-robin fashion. It works well for both categorical and numerical data.\n",
    "\n",
    "To demonstrate Multiple Imputation by Chained Equations (MICE) in Python, we can use the IterativeImputer class from the sklearn.impute module. MICE is a sophisticated method of imputation that models each feature with missing values as a function of other features, and it uses that estimate for imputation. It does this in a round-robin fashion: each feature is modeled in turn. The MICE algorithm is implemented in the IterativeImputer class.\n",
    "\n",
    "Let's see how to implement MICE in Python using the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imoprt libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# laod the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing values\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create a LabelEncoder object using LabelEncoder() in for loop for categorical columns\n",
    "# Columns to encode\n",
    "columns_to_encode = ['sex', 'embarked', 'who', 'deck', 'class', 'embark_town', 'alive']\n",
    "\n",
    "# Dictionary to store LabelEncoders for each column\n",
    "label_encoders = {}\n",
    "\n",
    "# Loop to apply LabelEncoder to each column for encoding\n",
    "for col in columns_to_encode:\n",
    "    # Create a new LabelEncoder for the column\n",
    "    le = LabelEncoder()\n",
    "    # Fit and transform the data\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    # Store the encoder in the dictionary\n",
    "    label_encoders[col] = le\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values with IterativeImputer\n",
    "# call the IterativeImputer class with max_iter = 10\n",
    "imputer = IterativeImputer(max_iter=10)\n",
    "\n",
    "#impute missing values using IterativeImputer in a for loop for age, embark_town,embarked columns and deck\n",
    "\n",
    "# Columns to impute\n",
    "columns_to_impute = ['age', 'embark_town', 'embarked', 'deck']\n",
    "\n",
    "# Loop to impute each column\n",
    "for col in columns_to_impute:\n",
    "    df[col] = imputer.fit_transform(df[[col]])    \n",
    "# check the missing values\n",
    "df.isnull().sum().sort_values(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform for encoded columns\n",
    "for col in columns_to_encode:\n",
    "    # Retrieve the corresponding LabelEncoder for the column\n",
    "    le = label_encoders[col]\n",
    "    # Inverse transform the data and convert to integer type\n",
    "    df[col] = le.inverse_transform(df[col].astype(int))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Deep Learning Methods\n",
    "\n",
    "Neural networks, especially autoencoders, can be effective in imputing missing values in complex datasets.\n",
    "Deep learning methods, particularly neural networks like autoencoders, offer a powerful approach for imputing missing values in complex datasets. These methods are especially useful when the data has intricate, non-linear relationships that traditional statistical methods might not capture effectively.\n",
    "\n",
    "### Understanding Autoencoders for Imputation:\n",
    "\n",
    "1. **What is an Autoencoder?**\n",
    "   - An autoencoder is a type of neural network that is trained to copy its input to its output.\n",
    "   - It has a hidden layer that describes a code used to represent the input.\n",
    "   - The network may be viewed as consisting of two parts: an encoder function, which compresses the input into a latent-space representation, and a decoder function, which reconstructs the input from the latent space.\n",
    "\n",
    "2. **How Autoencoders Work for Imputation:**\n",
    "   - The key idea is to train the autoencoder to ignore the noise (missing values) in the input data.\n",
    "   - During training, inputs with missing values are presented, and the network learns to predict the missing values in a way that minimizes reconstruction error for known parts of the data.\n",
    "   - This results in the network learning a robust representation of the data, enabling it to make reasonable guesses about missing values.\n",
    "\n",
    "3. **Advantages of Using Autoencoders:**\n",
    "   - **Handling Complex Patterns:** They can capture non-linear relationships in the data, which is particularly useful for complex datasets.\n",
    "   - **Scalability:** They can handle large-scale datasets efficiently.\n",
    "   - **Flexibility:** They can be adapted to different types of data (e.g., images, text, time-series).\n",
    "\n",
    "4. **Implementation Considerations:**\n",
    "   - **Data Preprocessing:** Data should be normalized or standardized before feeding it into an autoencoder.\n",
    "   - **Network Architecture:** The choice of architecture (number of layers, type of layers, etc.) depends on the complexity of the data.\n",
    "   - **Training Process:** It might involve techniques like dropout or noise addition to improve the model's ability to handle missing data.\n",
    "\n",
    "5. **Example Use-Cases:**\n",
    "   - **Image Data:** Filling in missing pixels or reconstructing corrupted images.\n",
    "   - **Time-Series Data:** Imputing missing values in sequences like stock prices or weather data.\n",
    "   - **Tabular Data:** Handling missing entries in datasets used for machine learning.\n",
    "\n",
    "### Implementation Example:\n",
    "\n",
    "Here's a simplified example of how you might set up an autoencoder for imputation in Python using TensorFlow and Keras: (Check the next notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
