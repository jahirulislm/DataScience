{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Heart Disease Prediction\n",
    "\n",
    "<h1 style=\"font-family: 'poppins'; font-weight: bold; color: Green;\">üë®‚ÄçüíªAuthor: Dr. Muhammad Aamamr Tufail</h1>\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Profile-blue?style=for-the-badge&logo=github)](https://github.com/AammarTufail) \n",
    "[![Kaggle](https://img.shields.io/badge/Kaggle-Profile-blue?style=for-the-badge&logo=kaggle)](https://www.kaggle.com/muhammadaammartufail) \n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/dr-muhammad-aammar-tufail-02471213b/)  \n",
    "\n",
    "[![YouTube](https://img.shields.io/badge/YouTube-Profile-red?style=for-the-badge&logo=youtube)](https://www.youtube.com/@codanics) \n",
    "[![Facebook](https://img.shields.io/badge/Facebook-Profile-blue?style=for-the-badge&logo=facebook)](https://www.facebook.com/aammar.tufail) \n",
    "[![TikTok](https://img.shields.io/badge/TikTok-Profile-black?style=for-the-badge&logo=tiktok)](https://www.tiktok.com/@draammar)  \n",
    "\n",
    "[![Twitter/X](https://img.shields.io/badge/Twitter-Profile-blue?style=for-the-badge&logo=twitter)](https://twitter.com/aammar_tufail) \n",
    "[![Instagram](https://img.shields.io/badge/Instagram-Profile-blue?style=for-the-badge&logo=instagram)](https://www.instagram.com/aammartufail/) \n",
    "[![Email](https://img.shields.io/badge/Email-Contact%20Me-red?style=for-the-badge&logo=email)](mailto:aammar@codanics.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Data (About Dataset)\n",
    "## Context\n",
    "This is a multivariate type of dataset which means providing or involving a variety of separate mathematical or statistical variables, multivariate numerical data analysis. It is composed of 14 attributes which are age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise-induced angina, oldpeak ‚Äî ST depression induced by exercise relative to rest, the slope of the peak exercise ST segment, number of major vessels and Thalassemia. This database includes 76 attributes, but all published studies relate to the use of a subset of 14 of them. The Cleveland database is the only one used by ML researchers to date. One of the major tasks on this dataset is to predict based on the given attributes of a patient that whether that particular person has heart disease or not and other is the experimental task to diagnose and find out various insights from this dataset which could help in understanding the problem more.\n",
    "\n",
    "### Content\n",
    "#### Column Descriptions:\n",
    "* `id` (Unique id for each patient)\n",
    "* `age` (Age of the patient in years)\n",
    "* `origin` (place of study)\n",
    "* `sex` (Male/Female)\n",
    "* `cp` chest pain type \n",
    "  1. typical angina, \n",
    "  2. atypical angina, \n",
    "  3. non-anginal, \n",
    "  4. asymptomatic\n",
    "* `trestbps` resting blood pressure (resting blood pressure (in mm Hg on admission to the hospital))\n",
    "* `chol` (serum cholesterol in mg/dl)\n",
    "* `fbs` (if fasting blood sugar > 120 mg/dl)\n",
    "* `restecg` (resting electrocardiographic results)\n",
    "* -- `Values`: [normal, stt abnormality, lv hypertrophy]\n",
    "* `thalach`: maximum heart rate achieved\n",
    "* `exang`: exercise-induced angina (True/ False)\n",
    "* `oldpeak`: ST depression induced by exercise relative to rest\n",
    "* `slope`: the slope of the peak exercise ST segment\n",
    "* `ca`: number of major vessels (0-3) colored by fluoroscopy\n",
    "* `thal`: [normal; fixed defect; reversible defect]\n",
    "* `num`: the predicted attribute\n",
    "\n",
    "### Acknowledgements\n",
    "#### Creators:\n",
    "* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n",
    "#### Relevant Papers:\n",
    "* Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J., Sandhu, S., Guppy, K., Lee, S., & Froelicher, V. (1989). International application of a new probability algorithm for the diagnosis of coronary artery disease. American Journal of Cardiology, 64,304--310. \n",
    "* David W. Aha & Dennis Kibler. \"Instance-based prediction of heart-disease presence with the Cleveland database.\" \n",
    "* Gennari, J.H., Langley, P, & Fisher, D. (1989). Models of incremental concept formation. Artificial Intelligence, 40, 11--61.\n",
    "#### Citation Request:\n",
    "The authors of the databases have requested that any publications resulting from the use of the data include the names of the principal investigator responsible for the data collection at each institution. \n",
    "\n",
    "**They would be:**\n",
    "\n",
    "* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims and Objective:\n",
    "\n",
    "We will fill this after doing some exploratory Data Analyis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Let's start the project by impoprting all the libraries that we will need in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# 1. to handle the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to visualize the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# To preprocess the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# import iterative imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "#for classification tasks\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "#metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ignore warnings   \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from csv file placed locally in our pc\n",
    "df = pd.read_csv('heart_disease_uci.csv')\n",
    "\n",
    "# print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Explore each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the datatype of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data shpae\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id column\n",
    "df['id'].min(), df['id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age column\n",
    "df['age'].min(), df['age'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's summarie the age column\n",
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a histogram to see the distribution of age column\n",
    "sns.histplot(df['age'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean, median and mode of age column using sns\n",
    "sns.histplot(df['age'], kde=True)\n",
    "plt.axvline(df['age'].mean(), color='red')\n",
    "plt.axvline(df['age'].median(), color='green')\n",
    "plt.axvline(df['age'].mode()[0], color='blue')\n",
    "\n",
    "# print the value of mean, median and mode of age column\n",
    "print('Mean:', df['age'].mean())\n",
    "print('Median:', df['age'].median())\n",
    "print('Mode:', df['age'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the gender based distribution of the dataset for age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of age column using plotly and coloring this by sex\n",
    "\n",
    "fig = px.histogram(data_frame=df, x='age', color='sex')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the values of sex column\n",
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentages of male and female value counts in the data\n",
    "male_count = 726\n",
    "female_count = 194\n",
    "total_count = male_count + female_count\n",
    "\n",
    "# calculate percentages\n",
    "male_percentage = (male_count / total_count) * 100\n",
    "female_percentage = (female_count / total_count) * 100\n",
    "\n",
    "# display the results\n",
    "print(f\"Male percentage in the data: {male_percentage:.2f}%\")\n",
    "print(f\"Female Percentage in the data: {female_percentage:.2f}%\")\n",
    "\n",
    "# difference\n",
    "difference_percentage = ((male_count - female_count) / female_count) * 100\n",
    "print(f\"Males are {difference_percentage:.2f}% more than females in the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the values count of age column grouping by sex column\n",
    "df.groupby('sex')['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets deal with dataset column\n",
    "# find the unique values in dataset column\n",
    "df['dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique values count in dataset column\n",
    "df['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the countplot of dataset column\n",
    "# sns.countplot(data=df, x='dataset', hue = 'sex')\n",
    "\n",
    "# better plots with plotly\n",
    "fig = px.bar(df, x='dataset', color='sex')\n",
    "fig.show()\n",
    "\n",
    "# print the values count of dataset column grouped by sex\n",
    "print(df.groupby('sex')['dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of age column using plotly and coloring this by dataset column\n",
    "fig = px.histogram(data_frame=df, x='age', color='dataset')\n",
    "fig.show()\n",
    "\n",
    "# print the mean median and mode of age column grouped by dataset column\n",
    "print(f\"Mean of Data Set: {df.groupby('dataset')['age'].mean()}\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Median of Data Set: {df.groupby('dataset')['age'].median()}\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Mode of Data Set: {df.groupby('dataset')['age'].agg(pd.Series.mode)}\")\n",
    "print(\"-------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore cp (Chest Pain) column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value count of cp column\n",
    "df['cp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count plot of cp column by sex column\n",
    "sns.countplot(df, x='cp', hue='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count plot of cp column by dataset column\n",
    "sns.countplot(df, x='cp', hue='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the plot of age column grouped by cp column using plotly\n",
    "fig = px.histogram(data_frame=df, x='age', color='cp')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let'e explore the trestbps (resting blood pressure) column:\n",
    "\n",
    "The normal resting blood pressure is 120/80 mm Hg.\n",
    "\n",
    "`Write here, what will happen if the blood pressure is high or low and then you can bin the data based on those values.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the value counts of trestbps column\n",
    "df['trestbps'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histplot of trestbps column\n",
    "sns.histplot(df['trestbps'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow; font-size: 350%; font-weight: bold;\">Dealing with missing values</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make a function to deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "missing_data_cols = df.isnull().sum()[df.isnull().sum() > 0].index.tolist()\n",
    "missing_data_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['thal', 'ca', 'slope', 'exang', 'restecg','fbs', 'cp', 'sex', 'num']\n",
    "bool_cols = ['fbs', 'exang']\n",
    "numeric_cols = ['oldpeak', 'thalch', 'chol', 'trestbps', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to impute the missing values in thal column\n",
    "\n",
    "def impute_categorical_missing_data(passed_col):\n",
    "    \n",
    "    df_null = df[df[passed_col].isnull()]\n",
    "    df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "    X = df_not_null.drop(passed_col, axis=1)\n",
    "    y = df_not_null[passed_col]\n",
    "    \n",
    "    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    if passed_col in bool_cols:\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        \n",
    "    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"The feature '\"+ passed_col+ \"' has been imputed with\", round((acc_score * 100), 2), \"accuracy\\n\")\n",
    "\n",
    "    X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "    if len(df_null) > 0: \n",
    "        df_null[passed_col] = rf_classifier.predict(X)\n",
    "        if passed_col in bool_cols:\n",
    "            df_null[passed_col] = df_null[passed_col].map({0: False, 1: True})\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df_combined = pd.concat([df_not_null, df_null])\n",
    "    \n",
    "    return df_combined[passed_col]\n",
    "\n",
    "def impute_continuous_missing_data(passed_col):\n",
    "    \n",
    "    df_null = df[df[passed_col].isnull()]\n",
    "    df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "    X = df_not_null.drop(passed_col, axis=1)\n",
    "    y = df_not_null[passed_col]\n",
    "    \n",
    "    other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "    \n",
    "    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf_regressor = RandomForestRegressor()\n",
    "\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "    print(\"MAE =\", mean_absolute_error(y_test, y_pred), \"\\n\")\n",
    "    print(\"RMSE =\", mean_squared_error(y_test, y_pred, squared=False), \"\\n\")\n",
    "    print(\"R2 =\", r2_score(y_test, y_pred), \"\\n\")\n",
    "\n",
    "    X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "            X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "    for col in other_missing_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "            X[col] = imputed_values[:, 0]\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "    if len(df_null) > 0: \n",
    "        df_null[passed_col] = rf_regressor.predict(X)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df_combined = pd.concat([df_not_null, df_null])\n",
    "    \n",
    "    return df_combined[passed_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# impute missing values using our functions\n",
    "for col in missing_data_cols:\n",
    "    print(\"Missing Values\", col, \":\", str(round((df[col].isnull().sum() / len(df)) * 100, 2))+\"%\")\n",
    "    if col in categorical_cols:\n",
    "        df[col] = impute_categorical_missing_data(col)\n",
    "    elif col in numeric_cols:\n",
    "        df[col] = impute_continuous_missing_data(col)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow; font-size: 350%; font-weight: bold;\">Dealing with Outliers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create box plots for all numeric columns using for loop and subplot\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.boxplot(x=df[col], color=colors[i])\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make box plots of all the numeric columns using for loop and plotly\n",
    "fig = px.box(data_frame=df, y='age')\n",
    "fig.show()\n",
    "\n",
    "fig = px.box(data_frame=df, y='trestbps')\n",
    "fig.show()\n",
    "\n",
    "fig = px.box(data_frame=df, y='chol')\n",
    "fig.show()\n",
    "\n",
    "fig = px.box(data_frame=df, y='thalch')\n",
    "fig.show()\n",
    "\n",
    "fig = px.box(data_frame=df, y='oldpeak')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the row from df where trestbps value is 0\n",
    "df[df['trestbps'] == 0]\n",
    "# remove this row from data\n",
    "df = df[df['trestbps'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow; font-size: 350%; font-weight: bold;\">Machine Learning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Target Column is `num` which is the predicted attribute. We will use this column to predict the heart disease. \n",
    "The unique values in this column are: [0, 1, 2, 3, 4], which states that there are 5 types of heart diseases.\n",
    "* `0 = no heart disease`\n",
    "* `1 = mild heart disease`\n",
    "* `2 = moderate heart disease `\n",
    "* `3 = severe heart disease`\n",
    "* `4 = critical heart disease `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into X and y\n",
    "X = df.drop('num', axis=1)\n",
    "y = df['num']\n",
    "\n",
    "# encode X data using separate label encoder for all categorical columns and save it for inverse transform\n",
    "\n",
    "# `Task: Separate Encoder for all cat and object columns and inverse transform at the end`\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlist all the models that you will use to predict the heart disease. These models should be classifiers for multi-class classification.\n",
    "\n",
    "1. logistic regression\n",
    "2. KNN - (k nierest neibhor)\n",
    "3. NB - (neive base)\n",
    "4. SVM - (sport vector machine)\n",
    "5. Decision Tree\n",
    "6. Random Forest\n",
    "7. XGBoost\n",
    "8. GradientBoosting\n",
    "9. AdaBoost\n",
    "10. lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# impot pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('Support Vector Machine', SVC(random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Ada Boost', AdaBoostClassifier(random_state=42)),\n",
    "    ('XG Boost', XGBClassifier(random_state=42)),\n",
    "    ('Naive Bayes', GaussianNB())\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance\n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        # ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        # ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(\"Model:\", name)\n",
    "    print(\"Cross-validation Accuracy:\", mean_accuracy)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print()\n",
    "    \n",
    "    # Check if the current model has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = pipeline\n",
    "\n",
    "# Retrieve the best model\n",
    "print(\"Best Model:\", best_model)\n",
    "\n",
    "# save the best model\n",
    "import pickle\n",
    "pickle.dump(best_model, open('heart_disease_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputs:\n",
    "1. The minimum age to have a heart disease starts from 28 years old.\n",
    "2. Most of the people get heart disease at the age of 53-54 years.\n",
    "3. Most of the males and females get are with heart disease at the age of 54-55 years.\n",
    "4. Male percentage in the data: 78.91%\n",
    "5. Female Percentage in the data: 21.09%\n",
    "6. Males are 274.23% more than females in the data.\n",
    "7. We have highest number of people from Cleveland (304) and lowest from Switzerland (123).\n",
    "   1. The highest number of females in this dataset are from Cleveland (97) and lowest from VA Long Beach (6).\n",
    "   2. The highest number of males in this dataset are from Hungary (212) and lowest from Switzerland (113).\n",
    "8. `Write down the observations here about mean, median and mode of age column, grouped by dataset column.`\n",
    "9. `Write down the observation from cp column here.`\n",
    "10. `Write down the steps of Imputing missingvalues and why did you choose those steps?`\n",
    "11. `How did you deal with outliers, write the information Step by Step`.\n",
    "12. `Explore all other columns and write their outputs`.\n",
    "13. `write down the observations from model output.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment Alert:** How can you improve the accuracy of this or any other model?\n",
    "\n",
    "## Hint: Create a function to select the best hyper-parameter tuned model for multiclass classification on any kind of data, and then use that function to select the best model for this dataset.\n",
    "\n",
    "- Pipeline: Data preprocess, Feature Engineering, Feature Selection, Hyperparameter Tuning, Model Selection, application and evaluation, make figures.\n",
    "\n",
    "`Hints:`\n",
    "1. Feature Engineering\n",
    "2. Feature Selection instead of 14 use 12, 10, or 8, or 6, features.\n",
    "3. Data Pre-processing (Scaling and Normalization).\n",
    "4. Hyperparameter Tuning.\n",
    "   1. Ensemble Learning (Stacking, Bagging, Boosting) increase their depth.\n",
    "5. Use different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'poppins'; font-weight: bold; color: Green;\">üë®‚ÄçüíªAuthor: Dr. Muhammad Aamamr Tufail</h1>\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Profile-blue?style=for-the-badge&logo=github)](https://github.com/AammarTufail) \n",
    "[![Kaggle](https://img.shields.io/badge/Kaggle-Profile-blue?style=for-the-badge&logo=kaggle)](https://www.kaggle.com/muhammadaammartufail) \n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/dr-muhammad-aammar-tufail-02471213b/)  \n",
    "\n",
    "[![YouTube](https://img.shields.io/badge/YouTube-Profile-red?style=for-the-badge&logo=youtube)](https://www.youtube.com/@codanics) \n",
    "[![Facebook](https://img.shields.io/badge/Facebook-Profile-blue?style=for-the-badge&logo=facebook)](https://www.facebook.com/aammar.tufail) \n",
    "[![TikTok](https://img.shields.io/badge/TikTok-Profile-black?style=for-the-badge&logo=tiktok)](https://www.tiktok.com/@draammar)  \n",
    "\n",
    "[![Twitter/X](https://img.shields.io/badge/Twitter-Profile-blue?style=for-the-badge&logo=twitter)](https://twitter.com/aammar_tufail) \n",
    "[![Instagram](https://img.shields.io/badge/Instagram-Profile-blue?style=for-the-badge&logo=instagram)](https://www.instagram.com/aammartufail/) \n",
    "[![Email](https://img.shields.io/badge/Email-Contact%20Me-red?style=for-the-badge&logo=email)](mailto:aammar@codanics.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
